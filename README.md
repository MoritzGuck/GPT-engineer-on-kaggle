# GPT-engineer-on-kaggle
Testing the capabilities of GPT-engineer with GPT 3.5 Turbo to solve a kaggle machine learning competition.

## What is GPT-engineer?

The GPT-engineer package is a GPT-based tool to generate a complete codebase for a software project just by using specifications in human langauge. More info here: [GPT-engineer](https://github.com/AntonOsika/gpt-engineer). 

## How was GPT-engineer used in this project?

The project is organized into two branches. One with the original output from GPT-engineer (`GPT-engineer-output`) and one with the output after some manual refactoring (`human-adapted-output`). 

The prompt for our project can be found in the file `prompt`. After kicking off a job, GPT-engineer asks clarification questions. I adapted the prompt-file 3 times upon clarification questions from GPT-engineer. 

The logs from the clarification questions and the engineered prompts from the GPT-engineer tool can be found in the `memory/logs` folder.

## The generated code base

The resulting codebase is found in the `workspace` folder. The file `all_output.txt` contains explanations from GPT-engineer on why it generated the code it did.

The folders `data`and the other files are not generated by GPT-engineer. 

## Thoughts and feelings on the generated codebase

The original output of GPT-engineer did not execute at all. To see the other changes, compare the two branches `GPT-engineer-output`and `human-adapted-output`. The codebase was generated by the inferior GPT-3.5 Turob model. The GPT-4 model might have performed much better. 

#### General steps

I quickly read throught the explanations in the file `all_output.txt`. The explanations and steps make reasonable for the project. 

#### files

However, instead of the 6 files that GPT-engineer wanted to generate, it only generated 4. Luckily the code for the other two are in the file `all_output.txt`. I manually added the code to the codebase.
In general, the names of the files were not what the other modules imported and they were missing the .py extension. 

#### Getting the code to run

The code did not run in the beginning. These are the steps that needed to be taken to get it to run:

- No preprocessing of categorical variables was done. I added an ordinal encoder to the pipeline.
- The wrong dataset was chosen for model evaluation. I created and changed it to the evaluation set.
- The column "Machine Failure" was dropped during training, but not for evaluation, making the dataset incompatible. I dropped it for evaluation as well.

#### The pre-processing & model

No preprocessing was done. Since GPT is a language model, it has not possibility to analyse the data on its own and take decisions accordingly. The model was not finetuned (no hyperparameter tuning, no cross-validation, no feature selection). 

#### Performance

Since the solution is so simple the performance was not great. I handed it in after the deadline of the competition since I did not write the code myself. The model had an 0.94 AUC score on the test set. This ranks us at place around 1000 out of 1500.

## Final thoughts

Eventhough the model was very basic and the code did not run in the beginning, I am impressed by the capabilities of GPT-engineer. It generated a codebase with a clean structure that makes sense for a machine learning project. It did not make basic mistakes, except the incomplete creation of files. Some files did not need any adaption whatsoever. It is a great basis to work on as a data scientist. I am sure that the GPT-4 model will perform much better. 
